import logging
import os
import chromadb
import asyncio
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from typing import List, Optional
from pathlib import Path
import gdown

# --- 1. Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ùˆ Ø«Ø§Ø¨Øªâ€ŒÙ‡Ø§ ---
BASE_DATA_DIR = Path("/app/product_db")
DB_PATH = str(BASE_DATA_DIR)
COLLECTION_NAME = "products"
API_VERSION = "2.3.0-final-debug"
TOP_K_RESULTS = 15

# --- 2. Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù„Ø§Ú¯ ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- 3. Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø³Ø±Ø§Ø³Ø±ÛŒ ---
collection = None
app_initialized = False

# --- 4. Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Pydantic ---
class VectorSearchRequest(BaseModel):
    embedding: List[float] = Field(..., description="The pre-computed embedding vector for the query.")
    keywords: List[str] = Field(..., description="A list of keywords for initial filtering.", example=["rug", "velvet"])

class SearchResult(BaseModel):
    id: str
    name: str
    score: float

class DownloadRequest(BaseModel):
    drive_link: str
    destination_path: str = ""

class DebugRequest(BaseModel):
    product_name: str = Field(..., description="Ù†Ø§Ù… Ø¯Ù‚ÛŒÙ‚ ÙØ§Ø±Ø³ÛŒ Ù…Ø­ØµÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ.")

class DebugResponse(BaseModel):
    product_name: str
    embedding: List[float]


# --- 5. Ø³Ø§Ø®Øª Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù† FastAPI ---
app = FastAPI(
    title="Headless Vector Search API (Debug & Production Ready)",
    description="Provides endpoints for pure vector search, hybrid search, and debugging.",
    version=API_VERSION
)

# --- 6. ØªÙˆØ§Ø¨Ø¹ Ù‡Ù…Ø²Ù…Ø§Ù† (Sync) ---
def initialize_database_sync():
    """ÙÙ‚Ø· Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    global collection, app_initialized
    if not os.path.isdir(DB_PATH):
        raise FileNotFoundError("Database path does not exist. Please download the database first.")
    
    logger.info(f"--- Ø¯Ø± Ø­Ø§Ù„ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ ChromaDB Ø¯Ø± Ù…Ø³ÛŒØ±: {DB_PATH} ---")
    db_client = chromadb.PersistentClient(path=DB_PATH)
    collection = db_client.get_or_create_collection(name=COLLECTION_NAME)
    logger.info(f"âœ… Ø§ØªØµØ§Ù„ Ø¨Ù‡ ChromaDB Ù…ÙˆÙÙ‚ÛŒØªâ€ŒØ¢Ù…ÛŒØ² Ø¨ÙˆØ¯. Ú©Ø§Ù„Ú©Ø´Ù† '{COLLECTION_NAME}' Ø´Ø§Ù…Ù„ {collection.count()} Ø¢ÛŒØªÙ… Ø§Ø³Øª.")
    app_initialized = True


def search_sync_pure_vector(embedding: List[float]) -> List[SearchResult]:
    """
    Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆÚ©ØªÙˆØ±ÛŒ Ø®Ø§Ù„Øµ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ùˆ ÙÛŒÙ„ØªØ± Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ Ø±Ø§ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.
    """
    logger.info("Performing PURE vector search (keyword filter disabled).")
    vector_search_results = collection.query(
        query_embeddings=[embedding],
        n_results=TOP_K_RESULTS,
    )

    if not vector_search_results or not vector_search_results.get('ids')[0]:
        logger.warning("No results found from pure vector search.")
        return []

    final_results = []
    ids, distances, documents = vector_search_results['ids'][0], vector_search_results['distances'][0], vector_search_results['documents'][0]
    for i in range(len(ids)):
        score = (1 - distances[i]) * 100
        final_results.append({"id": ids[i], "name": documents[i] if documents else "N/A", "score": score})
    return final_results


def search_sync_hybrid(embedding: List[float], keywords: List[str]) -> List[SearchResult]:
    """
    Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ Ø¨Ø§ ØªÙ‚Ø³ÛŒÙ… Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©â€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³.
    """
    logger.info(f"Performing HYBRID search with {len(keywords)} keywords.")
    
    # Ø§Ú¯Ø± ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ Ú©Ù… Ø§Ø³ØªØŒ Ø§Ø² Ø±ÙˆØ´ Ù‚Ø¨Ù„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
    if len(keywords) <= KEYWORD_BATCH_SIZE:
        where_filter = {"$or": [{"$contains": kw} for kw in keywords]} if len(keywords) > 1 else {"$contains": keywords[0]}
        results_keyword = collection.get(where_document=where_filter, include=["documents"])
    else:
        # Ø§Ú¯Ø± ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ Ø²ÛŒØ§Ø¯ Ø§Ø³ØªØŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ù†
        logger.info(f"Keyword count exceeds batch size. Splitting into chunks of {KEYWORD_BATCH_SIZE}.")
        all_ids = set()
        
        # ØªÙ‚Ø³ÛŒÙ… Ù„ÛŒØ³Øª Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©â€ŒØªØ±
        keyword_batches = [keywords[i:i + KEYWORD_BATCH_SIZE] for i in range(0, len(keywords), KEYWORD_BATCH_SIZE)]
        
        for batch in keyword_batches:
            logger.info(f"Processing keyword batch with {len(batch)} items.")
            where_filter = {"$or": [{"$contains": kw} for kw in batch]}
            try:
                batch_results = collection.get(where_document=where_filter, include=[]) # ÙÙ‚Ø· Ø¢ÛŒâ€ŒØ¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ù„Ø§Ø²Ù… Ø¯Ø§Ø±ÛŒÙ…
                if batch_results and batch_results.get('ids'):
                    all_ids.update(batch_results['ids'])
            except Exception as e:
                logger.warning(f"A batch query failed, but continuing. Error: {e}")

        if not all_ids:
            logger.warning("No results found after keyword filtering stage.")
            return []
            
        # Ø³Ø§Ø®Øª ÛŒÚ© Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù†ØªÛŒØ¬Ù‡ Ø³Ø§Ø®ØªÚ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Ø¨Ù‚ÛŒÙ‡ Ú©Ø¯
        results_keyword = {'ids': list(all_ids)}

    if not results_keyword or not results_keyword.get('ids'):
        logger.warning("No results found after keyword filtering stage.")
        return []
        
    logger.info(f"Found {len(results_keyword['ids'])} unique results after keyword filter. Reranking...")
    
    # Ù…Ø±Ø­Ù„Ù‡ Ù†Ù‡Ø§ÛŒÛŒ: Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆÚ©ØªÙˆØ±ÛŒ Ø±ÙˆÛŒ Ø¢ÛŒâ€ŒØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù‡
    # Ù†Ú©ØªÙ‡: Ø§Ú¯Ø± ØªØ¹Ø¯Ø§Ø¯ Ø¢ÛŒâ€ŒØ¯ÛŒâ€ŒÙ‡Ø§ Ø®ÛŒÙ„ÛŒ Ø²ÛŒØ§Ø¯ Ø¨Ø§Ø´Ø¯ (Ù…Ø«Ù„Ø§ Ú†Ù†Ø¯ Ø¯Ù‡ Ù‡Ø²Ø§Ø±)ØŒ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù‡Ù… Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú©Ù†Ø¯ Ø´ÙˆØ¯
    # Ø§Ù…Ø§ Ù…Ø¹Ù…ÙˆÙ„Ø§ Ø¨Ø³ÛŒØ§Ø± Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø§Ø² Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…ØªÙ†ÛŒ Ø§Ø³Øª.
    vector_search_results = collection.query(
        query_embeddings=[embedding],
        n_results=TOP_K_RESULTS,
        where={"id": {"$in": results_keyword['ids']}}
    )

    if not vector_search_results or not vector_search_results.get('ids')[0]:
        logger.warning("No results found after reranking stage.")
        return []

    final_results = []
    ids, distances, documents = vector_search_results['ids'][0], vector_search_results['distances'][0], vector_search_results['documents'][0]
    for i in range(len(ids)):
        score = (1 - distances[i]) * 100
        final_results.append({"id": ids[i], "name": documents[i], "score": score})
    return final_results

def get_embedding_by_name_sync(product_name: str) -> Optional[List[float]]:
    """
    ÙˆÚ©ØªÙˆØ± Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯ ÛŒÚ© Ù…Ø­ØµÙˆÙ„ Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ø§Ù… Ø¯Ù‚ÛŒÙ‚ Ø¢Ù† Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    logger.info(f"Debugging: Attempting to retrieve product by name: '{product_name}'")
    result = collection.get(
        where_document={"$eq": product_name},
        include=["embeddings"]
    )
    if result and result.get('embeddings'):
        logger.info(f"âœ… Debug: Product '{product_name}' found.")
        return result['embeddings'][0]
    else:
        logger.warning(f"âš ï¸ Debug: Product '{product_name}' NOT found.")
        return None



def start_download(url: str, output_path: Path):
    """ØªØ§Ø¨Ø¹ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù‡ Ø¯Ø± Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯."""
    logger.info(f"ğŸš€ Ø´Ø±ÙˆØ¹ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø§Ø² {url} Ø¨Ù‡ {output_path}")
    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        gdown.download(url=url, output=str(output_path), quiet=False, fuzzy=True)
        logger.info(f"âœ… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¨Ø±Ø§ÛŒ {output_path} Ú©Ø§Ù…Ù„ Ø´Ø¯.")
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ {url}. Ø¯Ù„ÛŒÙ„: {e}")

# --- 7. Endpoint Ù‡Ø§ÛŒ API (Asynchronous) ---
@app.post("/startup/")
async def startup_server():
    """Endpoint Ø¨Ø±Ø§ÛŒ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³."""
    global app_initialized
    if app_initialized:
        return {"status": "warning", "message": "Application is already initialized."}
    try:
        await asyncio.get_running_loop().run_in_executor(None, initialize_database_sync)
        return {"status": "success", "message": "Database initialized successfully."}
    except Exception as e:
        app_initialized = False
        logger.critical(f"âŒ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to initialize database: {str(e)}")

@app.post("/get-files/")
async def schedule_download(request: DownloadRequest, background_tasks: BackgroundTasks):
    """Endpoint Ø¨Ø±Ø§ÛŒ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯."""
    full_path = BASE_DATA_DIR.joinpath(request.destination_path).resolve()
    if BASE_DATA_DIR not in full_path.parents and full_path != BASE_DATA_DIR:
        raise HTTPException(status_code=400, detail="Ø®Ø·Ø§: Ù…Ø³ÛŒØ± Ù…Ù‚ØµØ¯ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª.")
    background_tasks.add_task(start_download, request.drive_link, full_path)
    return {
        "status": "success",
        "message": "ÙˆØ¸ÛŒÙÙ‡ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯.",
        "details": {"drive_link": request.drive_link, "save_location": str(full_path)}
    }


@app.post("/vector-search/", response_model=List[SearchResult], summary="Pure Vector Search (Debug)")
async def vector_search(request: VectorSearchRequest):
    """
    Endpoint Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆÚ©ØªÙˆØ±ÛŒ Ø®Ø§Ù„Øµ (ÙÛŒÙ„ØªØ± Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ Ø±Ø§ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯).
    """
    if not app_initialized:
        raise HTTPException(status_code=503, detail="Service is not initialized. Please call /startup first.")
    
    try:
        loop = asyncio.get_running_loop()
        final_results = await loop.run_in_executor(None, search_sync_pure_vector, request.embedding)
        logger.info(f"Returning {len(final_results)} pure vector search results.")
        return final_results
    except Exception as e:
        logger.error(f"Error during pure vector search: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="An error occurred during the search process.")


@app.post("/hybrid-search/", response_model=List[SearchResult], summary="Hybrid Search (Production)")
async def hybrid_search(request: VectorSearchRequest):
    """Endpoint Ø§ØµÙ„ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ (Ø§Ø¨ØªØ¯Ø§ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ØŒ Ø³Ù¾Ø³ ÙˆÚ©ØªÙˆØ±)."""
    if not app_initialized:
        raise HTTPException(status_code=503, detail="Service is not initialized. Please call /startup first.")
    if not request.keywords:
        raise HTTPException(status_code=400, detail="Keywords list cannot be empty.")
    try:
        # âœ¨âœ¨âœ¨ Ø§ØµÙ„Ø§Ø­ Ø§ØµÙ„ÛŒ Ø§ÛŒÙ†Ø¬Ø§Ø³Øª: Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø±Ø§ Ø¨Ù‡ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú© ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒØ¯ âœ¨âœ¨âœ¨
        normalized_keywords = [kw.lower() for kw in request.keywords]

        loop = asyncio.get_running_loop()
        # Ø§Ø² Ù„ÛŒØ³Øª Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡ Ø¯Ø± ØªØ§Ø¨Ø¹ Ø¬Ø³ØªØ¬Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
        final_results = await loop.run_in_executor(None, search_sync_hybrid, request.embedding, normalized_keywords)
        
        logger.info(f"Returning {len(final_results)} hybrid search results.")
        return final_results
    except Exception as e:
        logger.error(f"Error during hybrid search: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="An error occurred during the search process.")


@app.post("/get-embedding/", response_model=DebugResponse, summary="Debug: Get Embedding by Name")
async def get_embedding_by_name(request: DebugRequest):
    """
    ÛŒÚ© endpoint Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ú©Ù‡ ÙˆÚ©ØªÙˆØ± Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù…Ø­ØµÙˆÙ„ Ø±Ø§ Ø¨Ø§ Ù†Ø§Ù… Ø¯Ù‚ÛŒÙ‚ Ø¢Ù† Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    """
    if not app_initialized:
        raise HTTPException(status_code=503, detail="Service is not initialized. Please call /startup first.")
    try:
        loop = asyncio.get_running_loop()
        embedding = await loop.run_in_executor(None, get_embedding_by_name_sync, request.product_name)
        if embedding:
            return {"product_name": request.product_name, "embedding": embedding}
        else:
            raise HTTPException(status_code=404, detail=f"Product '{request.product_name}' not found in the database.")
    except Exception as e:
        logger.error(f"Error during debug lookup: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="An error occurred during the debug lookup.")



@app.get("/", summary="Health Check")
async def read_root():
    """Endpoint Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª Ø³Ø±ÙˆÛŒØ³."""
    return {
        "status": "OK" if app_initialized else "Pending Initialization",
        "message": "Server is running. Call /startup to initialize model and DB.",
        "version": API_VERSION,
        "initialized": app_initialized
    }


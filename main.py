import os
import gdown
import asyncio
import logging
import chromadb
import numpy as np
from typing import List
from pathlib import Path
from typing import List, Optional
from pydantic import BaseModel, Field
from fastapi import FastAPI, HTTPException, BackgroundTasks


# --- 1. Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ùˆ Ø«Ø§Ø¨Øªâ€ŒÙ‡Ø§ ---
BASE_DATA_DIR = Path("/app/product_db")
DB_PATH = str(BASE_DATA_DIR)
COLLECTION_NAME = "products"
API_VERSION = "2.3.0-final-debug"
TOP_K_RESULTS = 15

# --- 2. Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù„Ø§Ú¯ ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- 3. Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø³Ø±Ø§Ø³Ø±ÛŒ ---
collection = None
app_initialized = False
KEYWORD_BATCH_SIZE = 100
ID_RERANK_BATCH_SIZE = 500

# --- 4. Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Pydantic ---
class VectorSearchRequest(BaseModel):
    embedding: List[float] = Field(..., description="The pre-computed embedding vector for the query.")
    keywords: List[str] = Field(..., description="A list of keywords for initial filtering.", example=["rug", "velvet"])

class SearchResult(BaseModel):
    id: str
    name: str
    score: float

class DownloadRequest(BaseModel):
    drive_link: str
    destination_path: str = ""

class DebugRequest(BaseModel):
    product_name: str = Field(..., description="Ù†Ø§Ù… Ø¯Ù‚ÛŒÙ‚ ÙØ§Ø±Ø³ÛŒ Ù…Ø­ØµÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ.")

class DebugResponse(BaseModel):
    product_name: str
    embedding: List[float]


# --- 5. Ø³Ø§Ø®Øª Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù† FastAPI ---
app = FastAPI(
    title="Headless Vector Search API (Debug & Production Ready)",
    description="Provides endpoints for pure vector search, hybrid search, and debugging.",
    version=API_VERSION
)

# --- 6. ØªÙˆØ§Ø¨Ø¹ Ù‡Ù…Ø²Ù…Ø§Ù† (Sync) ---
def initialize_database_sync():
    """ÙÙ‚Ø· Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    global collection, app_initialized
    if not os.path.isdir(DB_PATH):
        raise FileNotFoundError("Database path does not exist. Please download the database first.")
    
    logger.info(f"--- Ø¯Ø± Ø­Ø§Ù„ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ ChromaDB Ø¯Ø± Ù…Ø³ÛŒØ±: {DB_PATH} ---")
    db_client = chromadb.PersistentClient(path=DB_PATH)
    collection = db_client.get_or_create_collection(name=COLLECTION_NAME)
    logger.info(f"âœ… Ø§ØªØµØ§Ù„ Ø¨Ù‡ ChromaDB Ù…ÙˆÙÙ‚ÛŒØªâ€ŒØ¢Ù…ÛŒØ² Ø¨ÙˆØ¯. Ú©Ø§Ù„Ú©Ø´Ù† '{COLLECTION_NAME}' Ø´Ø§Ù…Ù„ {collection.count()} Ø¢ÛŒØªÙ… Ø§Ø³Øª.")
    app_initialized = True


def search_sync_pure_vector(embedding: List[float]) -> List[SearchResult]:
    """
    Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆÚ©ØªÙˆØ±ÛŒ Ø®Ø§Ù„Øµ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ùˆ ÙÛŒÙ„ØªØ± Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ Ø±Ø§ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.
    """
    logger.info("Performing PURE vector search (keyword filter disabled).")
    vector_search_results = collection.query(
        query_embeddings=[embedding],
        n_results=TOP_K_RESULTS,
    )

    if not vector_search_results or not vector_search_results.get('ids')[0]:
        logger.warning("No results found from pure vector search.")
        return []

    final_results = []
    ids, distances, documents = vector_search_results['ids'][0], vector_search_results['distances'][0], vector_search_results['documents'][0]
    for i in range(len(ids)):
        score = (1 - distances[i]) * 100
        final_results.append({"id": ids[i], "name": documents[i] if documents else "N/A", "score": score})
    return final_results


def search_sync_hybrid(embedding: List[float], keywords: List[str]) -> List[SearchResult]:
    """
    Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ Ú©Ø§Ù…Ù„Ø§Ù‹ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§ÛŒ "too many SQL variables"
    Ø¨Ø¯ÙˆÙ† Ø§Ø² Ø¯Ø³Øª Ø¯Ø§Ø¯Ù† Ù‡ÛŒÚ† Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ.
    """
    logger.info(f"Performing FULLY BATCHED HYBRID search with {len(keywords)} keywords.")
    
    # --- Ù…Ø±Ø­Ù„Ù‡ Û±: ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¨Ø§ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡ ---
    all_ids_from_keyword_filter = set()
    keyword_batches = [keywords[i:i + KEYWORD_BATCH_SIZE] for i in range(0, len(keywords), KEYWORD_BATCH_SIZE)]
    
    for batch in keyword_batches:
        where_filter = {"$or": [{"$contains": kw} for kw in batch]} if len(batch) > 1 else {"$contains": batch[0]}
        try:
            # ÙÙ‚Ø· Ø¢ÛŒâ€ŒØ¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ… ØªØ§ Ø­Ø¬Ù… Ø¯Ø§Ø¯Ù‡ Ú©Ù…ØªØ± Ø¨Ø§Ø´Ø¯
            batch_results = collection.get(where_document=where_filter, include=[])
            if batch_results and batch_results.get('ids'):
                all_ids_from_keyword_filter.update(batch_results['ids'])
        except Exception as e:
            logger.warning(f"A keyword batch query failed, but continuing. Error: {e}")

    if not all_ids_from_keyword_filter:
        logger.warning("No results found after keyword filtering stage.")
        return []
    
    unique_ids = list(all_ids_from_keyword_filter)
    logger.info(f"Found {len(unique_ids)} unique results after keyword filter. Reranking in batches...")

    # --- Ù…Ø±Ø­Ù„Ù‡ Û²: Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø¬Ø¯Ø¯ (Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆÚ©ØªÙˆØ±ÛŒ) Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡ ---
    all_final_results = []
    id_batches = [unique_ids[i:i + ID_RERANK_BATCH_SIZE] for i in range(0, len(unique_ids), ID_RERANK_BATCH_SIZE)]

    for id_batch in id_batches:
        try:
            vector_search_results = collection.query(
                query_embeddings=[embedding],
                n_results=len(id_batch), # ØªÙ…Ø§Ù… Ù†ØªØ§ÛŒØ¬ Ø§ÛŒÙ† Ø¯Ø³ØªÙ‡ Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ…
                where={"id": {"$in": id_batch}}
            )

            if vector_search_results and vector_search_results.get('ids')[0]:
                ids = vector_search_results['ids'][0]
                distances = vector_search_results['distances'][0]
                documents = vector_search_results['documents'][0]
                for i in range(len(ids)):
                    score = (1 - distances[i]) * 100
                    all_final_results.append({"id": ids[i], "name": documents[i], "score": score})
        except Exception as e:
            logger.warning(f"An ID batch query for reranking failed, but continuing. Error: {e}")
            
    # --- Ù…Ø±Ø­Ù„Ù‡ Û³: Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªØ§ÛŒØ¬ ---
    if not all_final_results:
        logger.warning("No results found after reranking stage.")
        return []

    # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ ØªÙ…Ø§Ù… Ù†ØªØ§ÛŒØ¬ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø² (score) Ø¨Ù‡ ØµÙˆØ±Øª Ù†Ø²ÙˆÙ„ÛŒ
    all_final_results.sort(key=lambda x: x['score'], reverse=True)
    
    # Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† N Ù†ØªÛŒØ¬Ù‡ Ø¨Ø±ØªØ± (TOP_K_RESULTS)
    top_results = all_final_results[:TOP_K_RESULTS]
    
    logger.info(f"Returning {len(top_results)} final hybrid search results after sorting all candidates.")
    return top_results

def get_embedding_by_name_sync(product_name: str) -> Optional[List[float]]:
    """
    ÙˆÚ©ØªÙˆØ± Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯ ÛŒÚ© Ù…Ø­ØµÙˆÙ„ Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ø§Ù… Ø¯Ù‚ÛŒÙ‚ Ø¢Ù† Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    logger.info(f"Debugging: Attempting to retrieve product by name: '{product_name}'")
    result = collection.get(
        where_document={"$eq": product_name},
        include=["embeddings"]
    )
    if result and result.get('embeddings'):
        logger.info(f"âœ… Debug: Product '{product_name}' found.")
        return result['embeddings'][0]
    else:
        logger.warning(f"âš ï¸ Debug: Product '{product_name}' NOT found.")
        return None



def start_download(url: str, output_path: Path):
    """ØªØ§Ø¨Ø¹ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù‡ Ø¯Ø± Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯."""
    logger.info(f"ğŸš€ Ø´Ø±ÙˆØ¹ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø§Ø² {url} Ø¨Ù‡ {output_path}")
    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        gdown.download(url=url, output=str(output_path), quiet=False, fuzzy=True)
        logger.info(f"âœ… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¨Ø±Ø§ÛŒ {output_path} Ú©Ø§Ù…Ù„ Ø´Ø¯.")
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ {url}. Ø¯Ù„ÛŒÙ„: {e}")

# --- 7. Endpoint Ù‡Ø§ÛŒ API (Asynchronous) ---
@app.post("/startup/")
async def startup_server():
    """Endpoint Ø¨Ø±Ø§ÛŒ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³."""
    global app_initialized
    if app_initialized:
        return {"status": "warning", "message": "Application is already initialized."}
    try:
        await asyncio.get_running_loop().run_in_executor(None, initialize_database_sync)
        return {"status": "success", "message": "Database initialized successfully."}
    except Exception as e:
        app_initialized = False
        logger.critical(f"âŒ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to initialize database: {str(e)}")

@app.post("/get-files/")
async def schedule_download(request: DownloadRequest, background_tasks: BackgroundTasks):
    """Endpoint Ø¨Ø±Ø§ÛŒ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯."""
    full_path = BASE_DATA_DIR.joinpath(request.destination_path).resolve()
    if BASE_DATA_DIR not in full_path.parents and full_path != BASE_DATA_DIR:
        raise HTTPException(status_code=400, detail="Ø®Ø·Ø§: Ù…Ø³ÛŒØ± Ù…Ù‚ØµØ¯ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª.")
    background_tasks.add_task(start_download, request.drive_link, full_path)
    return {
        "status": "success",
        "message": "ÙˆØ¸ÛŒÙÙ‡ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯.",
        "details": {"drive_link": request.drive_link, "save_location": str(full_path)}
    }


@app.post("/vector-search/", response_model=List[SearchResult], summary="Pure Vector Search (Debug)")
async def vector_search(request: VectorSearchRequest):
    """
    Endpoint Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆÚ©ØªÙˆØ±ÛŒ Ø®Ø§Ù„Øµ (ÙÛŒÙ„ØªØ± Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ Ø±Ø§ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯).
    """
    if not app_initialized:
        raise HTTPException(status_code=503, detail="Service is not initialized. Please call /startup first.")
    
    try:
        loop = asyncio.get_running_loop()
        final_results = await loop.run_in_executor(None, search_sync_pure_vector, request.embedding)
        logger.info(f"Returning {len(final_results)} pure vector search results.")
        return final_results
    except Exception as e:
        logger.error(f"Error during pure vector search: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="An error occurred during the search process.")


@app.post("/hybrid-search/", response_model=List[SearchResult])
async def hybrid_search(request: VectorSearchRequest):
    logger.info(f"Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¬Ø³ØªØ¬ÙˆÛŒ Ù‡ÛŒØ¨Ø±ÛŒØ¯ÛŒ Ø¨Ø§ {len(request.keywords)} Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯.")

    if collection is None:
        raise HTTPException(status_code=503, detail="Ø³Ø±ÙˆÛŒØ³ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª: Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.")

    if not request.keywords:
        raise HTTPException(status_code=400, detail="Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø®Ø§Ù„ÛŒ Ø¨Ø§Ø´Ø¯.")

    try:
        # Ù…Ø±Ø­Ù„Ù‡ Û±: ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¨Ø§ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±)
        all_ids_from_keyword_filter = set()
        normalized_keywords = [kw.lower() for kw in request.keywords]
        keyword_batches = [normalized_keywords[i:i + KEYWORD_BATCH_SIZE] for i in range(0, len(normalized_keywords), KEYWORD_BATCH_SIZE)]
        
        for batch in keyword_batches:
            where_filter = {"$or": [{"$contains": kw} for kw in batch]} if len(batch) > 1 else {"$contains": batch[0]}
            try:
                batch_results = collection.get(where_document=where_filter, include=[])
                if batch_results and batch_results.get('ids'):
                    all_ids_from_keyword_filter.update(batch_results['ids'])
            except Exception:
                pass

        if not all_ids_from_keyword_filter:
            return []
        
        unique_ids = list(all_ids_from_keyword_filter)
        logger.info(f"{len(unique_ids)} Ù†ØªÛŒØ¬Ù‡ Ù¾Ø³ Ø§Ø² ÙÛŒÙ„ØªØ± ÛŒØ§ÙØª Ø´Ø¯. Ø¯Ø± Ø­Ø§Ù„ ÙˆØ§Ú©Ø´ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª...")

        # Ù…Ø±Ø­Ù„Ù‡ Û²: Ú¯Ø±ÙØªÙ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù…Ù„ Ù…Ø­ØµÙˆÙ„Ø§Øª ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡
        results_keyword = collection.get(ids=unique_ids, include=["documents", "embeddings"])

        if not results_keyword or not results_keyword.get('ids'):
            return []

        logger.info(f"Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ {len(results_keyword['ids'])} Ù…Ø­ØµÙˆÙ„ Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯. Ø¯Ø± Ø­Ø§Ù„ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø¬Ø¯Ø¯...")

        # --- âœ¨âœ¨âœ¨ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨Ø§ NumPy âœ¨âœ¨âœ¨ ---
        
        # Ù…Ø±Ø­Ù„Ù‡ Û³: Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ùˆ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø¬Ø¯Ø¯ Ø¨Ø§ NumPy
        query_embedding = np.array(request.embedding, dtype=np.float32)
        filtered_embeddings = np.array(results_keyword['embeddings'], dtype=np.float32)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ø¨Ù‡ÛŒÙ†Ù‡
        # 1. Ù†Ø±Ù…Ø§Ù„Ø§ÛŒØ² Ú©Ø±Ø¯Ù† ÙˆÚ©ØªÙˆØ±Ù‡Ø§
        query_norm = query_embedding / np.linalg.norm(query_embedding)
        filtered_norms = filtered_embeddings / np.linalg.norm(filtered_embeddings, axis=1, keepdims=True)
        # 2. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¶Ø±Ø¨ Ø¯Ø§Ø®Ù„ÛŒ (Ú©Ù‡ Ø­Ø§Ù„Ø§ Ù…Ø¹Ø§Ø¯Ù„ Ø´Ø¨Ø§Ù‡Øª Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ Ø§Ø³Øª)
        similarities = np.dot(filtered_norms, query_norm)

        reranked_results = []
        for i, doc_name in enumerate(results_keyword['documents']):
            reranked_results.append({
                "id": results_keyword['ids'][i],
                "name": doc_name,
                "score": similarities[i] * 100
            })
        
        # Ù…Ø±Ø­Ù„Ù‡ Û´: Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ†â€ŒÙ‡Ø§
        reranked_results.sort(key=lambda x: x['score'], reverse=True)
        top_results = reranked_results[:TOP_K_RESULTS]
        
        logger.info(f"Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† {len(top_results)} Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ.")
        return top_results
    
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ÛŒÚ© Ø®Ø·Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ Ø¯Ø± Ø³Ø±ÙˆØ± Ø±Ø® Ø¯Ø§Ø¯.")



@app.post("/get-embedding/", response_model=DebugResponse, summary="Debug: Get Embedding by Name")
async def get_embedding_by_name(request: DebugRequest):
    """
    ÛŒÚ© endpoint Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ú©Ù‡ ÙˆÚ©ØªÙˆØ± Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù…Ø­ØµÙˆÙ„ Ø±Ø§ Ø¨Ø§ Ù†Ø§Ù… Ø¯Ù‚ÛŒÙ‚ Ø¢Ù† Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    """
    if not app_initialized:
        raise HTTPException(status_code=503, detail="Service is not initialized. Please call /startup first.")
    try:
        loop = asyncio.get_running_loop()
        embedding = await loop.run_in_executor(None, get_embedding_by_name_sync, request.product_name)
        if embedding:
            return {"product_name": request.product_name, "embedding": embedding}
        else:
            raise HTTPException(status_code=404, detail=f"Product '{request.product_name}' not found in the database.")
    except Exception as e:
        logger.error(f"Error during debug lookup: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="An error occurred during the debug lookup.")



@app.get("/", summary="Health Check")
async def read_root():
    """Endpoint Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª Ø³Ø±ÙˆÛŒØ³."""
    return {
        "status": "OK" if app_initialized else "Pending Initialization",
        "message": "Server is running. Call /startup to initialize model and DB.",
        "version": API_VERSION,
        "initialized": app_initialized
    }

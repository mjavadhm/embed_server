import logging
import os
import chromadb
import asyncio
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from typing import List, Optional
from pathlib import Path
import gdown

# --- 1. Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ùˆ Ø«Ø§Ø¨Øªâ€ŒÙ‡Ø§ ---
BASE_DATA_DIR = Path("/app/product_db")
DB_PATH = str(BASE_DATA_DIR)
COLLECTION_NAME = "products"
API_VERSION = "2.3.0-final-debug"
TOP_K_RESULTS = 15

# --- 2. Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù„Ø§Ú¯ ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- 3. Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø³Ø±Ø§Ø³Ø±ÛŒ ---
collection = None
app_initialized = False
KEYWORD_BATCH_SIZE = 100
ID_RERANK_BATCH_SIZE = 500

# --- 4. Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Pydantic ---
class VectorSearchRequest(BaseModel):
    embedding: List[float] = Field(..., description="The pre-computed embedding vector for the query.")
    keywords: List[str] = Field(..., description="A list of keywords for initial filtering.", example=["rug", "velvet"])

class SearchResult(BaseModel):
    id: str
    name: str
    score: float

class DownloadRequest(BaseModel):
    drive_link: str
    destination_path: str = ""

class DebugRequest(BaseModel):
    product_name: str = Field(..., description="Ù†Ø§Ù… Ø¯Ù‚ÛŒÙ‚ ÙØ§Ø±Ø³ÛŒ Ù…Ø­ØµÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ.")

class DebugResponse(BaseModel):
    product_name: str
    embedding: List[float]


# --- 5. Ø³Ø§Ø®Øª Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù† FastAPI ---
app = FastAPI(
    title="Headless Vector Search API (Debug & Production Ready)",
    description="Provides endpoints for pure vector search, hybrid search, and debugging.",
    version=API_VERSION
)

# --- 6. ØªÙˆØ§Ø¨Ø¹ Ù‡Ù…Ø²Ù…Ø§Ù† (Sync) ---
def initialize_database_sync():
    """ÙÙ‚Ø· Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    global collection, app_initialized
    if not os.path.isdir(DB_PATH):
        raise FileNotFoundError("Database path does not exist. Please download the database first.")
    
    logger.info(f"--- Ø¯Ø± Ø­Ø§Ù„ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ ChromaDB Ø¯Ø± Ù…Ø³ÛŒØ±: {DB_PATH} ---")
    db_client = chromadb.PersistentClient(path=DB_PATH)
    collection = db_client.get_or_create_collection(name=COLLECTION_NAME)
    logger.info(f"âœ… Ø§ØªØµØ§Ù„ Ø¨Ù‡ ChromaDB Ù…ÙˆÙÙ‚ÛŒØªâ€ŒØ¢Ù…ÛŒØ² Ø¨ÙˆØ¯. Ú©Ø§Ù„Ú©Ø´Ù† '{COLLECTION_NAME}' Ø´Ø§Ù…Ù„ {collection.count()} Ø¢ÛŒØªÙ… Ø§Ø³Øª.")
    app_initialized = True


def search_sync_pure_vector(embedding: List[float]) -> List[SearchResult]:
    """
    Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆÚ©ØªÙˆØ±ÛŒ Ø®Ø§Ù„Øµ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ùˆ ÙÛŒÙ„ØªØ± Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ Ø±Ø§ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.
    """
    logger.info("Performing PURE vector search (keyword filter disabled).")
    vector_search_results = collection.query(
        query_embeddings=[embedding],
        n_results=TOP_K_RESULTS,
    )

    if not vector_search_results or not vector_search_results.get('ids')[0]:
        logger.warning("No results found from pure vector search.")
        return []

    final_results = []
    ids, distances, documents = vector_search_results['ids'][0], vector_search_results['distances'][0], vector_search_results['documents'][0]
    for i in range(len(ids)):
        score = (1 - distances[i]) * 100
        final_results.append({"id": ids[i], "name": documents[i] if documents else "N/A", "score": score})
    return final_results


def search_sync_hybrid(embedding: List[float], keywords: List[str]) -> List[SearchResult]:
    """
    Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ Ú©Ø§Ù…Ù„Ø§Ù‹ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§ÛŒ "too many SQL variables"
    Ø¨Ø¯ÙˆÙ† Ø§Ø² Ø¯Ø³Øª Ø¯Ø§Ø¯Ù† Ù‡ÛŒÚ† Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ.
    """
    logger.info(f"Performing FULLY BATCHED HYBRID search with {len(keywords)} keywords.")
    
    # --- Ù…Ø±Ø­Ù„Ù‡ Û±: ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¨Ø§ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡ ---
    all_ids_from_keyword_filter = set()
    keyword_batches = [keywords[i:i + KEYWORD_BATCH_SIZE] for i in range(0, len(keywords), KEYWORD_BATCH_SIZE)]
    
    for batch in keyword_batches:
        where_filter = {"$or": [{"$contains": kw} for kw in batch]} if len(batch) > 1 else {"$contains": batch[0]}
        try:
            # ÙÙ‚Ø· Ø¢ÛŒâ€ŒØ¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ… ØªØ§ Ø­Ø¬Ù… Ø¯Ø§Ø¯Ù‡ Ú©Ù…ØªØ± Ø¨Ø§Ø´Ø¯
            batch_results = collection.get(where_document=where_filter, include=[])
            if batch_results and batch_results.get('ids'):
                all_ids_from_keyword_filter.update(batch_results['ids'])
        except Exception as e:
            logger.warning(f"A keyword batch query failed, but continuing. Error: {e}")

    if not all_ids_from_keyword_filter:
        logger.warning("No results found after keyword filtering stage.")
        return []
    
    unique_ids = list(all_ids_from_keyword_filter)
    logger.info(f"Found {len(unique_ids)} unique results after keyword filter. Reranking in batches...")

    # --- Ù…Ø±Ø­Ù„Ù‡ Û²: Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø¬Ø¯Ø¯ (Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆÚ©ØªÙˆØ±ÛŒ) Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡ ---
    all_final_results = []
    id_batches = [unique_ids[i:i + ID_RERANK_BATCH_SIZE] for i in range(0, len(unique_ids), ID_RERANK_BATCH_SIZE)]

    for id_batch in id_batches:
        try:
            vector_search_results = collection.query(
                query_embeddings=[embedding],
                n_results=len(id_batch), # ØªÙ…Ø§Ù… Ù†ØªØ§ÛŒØ¬ Ø§ÛŒÙ† Ø¯Ø³ØªÙ‡ Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ…
                where={"id": {"$in": id_batch}}
            )

            if vector_search_results and vector_search_results.get('ids')[0]:
                ids = vector_search_results['ids'][0]
                distances = vector_search_results['distances'][0]
                documents = vector_search_results['documents'][0]
                for i in range(len(ids)):
                    score = (1 - distances[i]) * 100
                    all_final_results.append({"id": ids[i], "name": documents[i], "score": score})
        except Exception as e:
            logger.warning(f"An ID batch query for reranking failed, but continuing. Error: {e}")
            
    # --- Ù…Ø±Ø­Ù„Ù‡ Û³: Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªØ§ÛŒØ¬ ---
    if not all_final_results:
        logger.warning("No results found after reranking stage.")
        return []

    # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ ØªÙ…Ø§Ù… Ù†ØªØ§ÛŒØ¬ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø² (score) Ø¨Ù‡ ØµÙˆØ±Øª Ù†Ø²ÙˆÙ„ÛŒ
    all_final_results.sort(key=lambda x: x['score'], reverse=True)
    
    # Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† N Ù†ØªÛŒØ¬Ù‡ Ø¨Ø±ØªØ± (TOP_K_RESULTS)
    top_results = all_final_results[:TOP_K_RESULTS]
    
    logger.info(f"Returning {len(top_results)} final hybrid search results after sorting all candidates.")
    return top_results

def get_embedding_by_name_sync(product_name: str) -> Optional[List[float]]:
    """
    ÙˆÚ©ØªÙˆØ± Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯ ÛŒÚ© Ù…Ø­ØµÙˆÙ„ Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ø§Ù… Ø¯Ù‚ÛŒÙ‚ Ø¢Ù† Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    logger.info(f"Debugging: Attempting to retrieve product by name: '{product_name}'")
    result = collection.get(
        where_document={"$eq": product_name},
        include=["embeddings"]
    )
    if result and result.get('embeddings'):
        logger.info(f"âœ… Debug: Product '{product_name}' found.")
        return result['embeddings'][0]
    else:
        logger.warning(f"âš ï¸ Debug: Product '{product_name}' NOT found.")
        return None



def start_download(url: str, output_path: Path):
    """ØªØ§Ø¨Ø¹ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù‡ Ø¯Ø± Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯."""
    logger.info(f"ğŸš€ Ø´Ø±ÙˆØ¹ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø§Ø² {url} Ø¨Ù‡ {output_path}")
    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        gdown.download(url=url, output=str(output_path), quiet=False, fuzzy=True)
        logger.info(f"âœ… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¨Ø±Ø§ÛŒ {output_path} Ú©Ø§Ù…Ù„ Ø´Ø¯.")
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ {url}. Ø¯Ù„ÛŒÙ„: {e}")

# --- 7. Endpoint Ù‡Ø§ÛŒ API (Asynchronous) ---
@app.post("/startup/")
async def startup_server():
    """Endpoint Ø¨Ø±Ø§ÛŒ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³."""
    global app_initialized
    if app_initialized:
        return {"status": "warning", "message": "Application is already initialized."}
    try:
        await asyncio.get_running_loop().run_in_executor(None, initialize_database_sync)
        return {"status": "success", "message": "Database initialized successfully."}
    except Exception as e:
        app_initialized = False
        logger.critical(f"âŒ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to initialize database: {str(e)}")

@app.post("/get-files/")
async def schedule_download(request: DownloadRequest, background_tasks: BackgroundTasks):
    """Endpoint Ø¨Ø±Ø§ÛŒ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯."""
    full_path = BASE_DATA_DIR.joinpath(request.destination_path).resolve()
    if BASE_DATA_DIR not in full_path.parents and full_path != BASE_DATA_DIR:
        raise HTTPException(status_code=400, detail="Ø®Ø·Ø§: Ù…Ø³ÛŒØ± Ù…Ù‚ØµØ¯ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª.")
    background_tasks.add_task(start_download, request.drive_link, full_path)
    return {
        "status": "success",
        "message": "ÙˆØ¸ÛŒÙÙ‡ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯.",
        "details": {"drive_link": request.drive_link, "save_location": str(full_path)}
    }


@app.post("/vector-search/", response_model=List[SearchResult], summary="Pure Vector Search (Debug)")
async def vector_search(request: VectorSearchRequest):
    """
    Endpoint Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆÚ©ØªÙˆØ±ÛŒ Ø®Ø§Ù„Øµ (ÙÛŒÙ„ØªØ± Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ Ø±Ø§ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯).
    """
    if not app_initialized:
        raise HTTPException(status_code=503, detail="Service is not initialized. Please call /startup first.")
    
    try:
        loop = asyncio.get_running_loop()
        final_results = await loop.run_in_executor(None, search_sync_pure_vector, request.embedding)
        logger.info(f"Returning {len(final_results)} pure vector search results.")
        return final_results
    except Exception as e:
        logger.error(f"Error during pure vector search: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="An error occurred during the search process.")


@app.post("/hybrid-search/", response_model=List[SearchResult], summary="Hybrid Search (Production)")
async def hybrid_search(request: VectorSearchRequest):
    """Endpoint Ø§ØµÙ„ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ (Ø§Ø¨ØªØ¯Ø§ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ØŒ Ø³Ù¾Ø³ ÙˆÚ©ØªÙˆØ±)."""
    if not app_initialized:
        raise HTTPException(status_code=503, detail="Service is not initialized. Please call /startup first.")
    if not request.keywords:
        raise HTTPException(status_code=400, detail="Keywords list cannot be empty.")
    try:
        # âœ¨âœ¨âœ¨ Ø§ØµÙ„Ø§Ø­ Ø§ØµÙ„ÛŒ Ø§ÛŒÙ†Ø¬Ø§Ø³Øª: Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø±Ø§ Ø¨Ù‡ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú© ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒØ¯ âœ¨âœ¨âœ¨
        normalized_keywords = [kw.lower() for kw in request.keywords]

        loop = asyncio.get_running_loop()
        # Ø§Ø² Ù„ÛŒØ³Øª Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡ Ø¯Ø± ØªØ§Ø¨Ø¹ Ø¬Ø³ØªØ¬Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
        final_results = await loop.run_in_executor(None, search_sync_hybrid, request.embedding, normalized_keywords)
        
        logger.info(f"Returning {len(final_results)} hybrid search results.")
        return final_results
    except Exception as e:
        logger.error(f"Error during hybrid search: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="An error occurred during the search process.")


@app.post("/get-embedding/", response_model=DebugResponse, summary="Debug: Get Embedding by Name")
async def get_embedding_by_name(request: DebugRequest):
    """
    ÛŒÚ© endpoint Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ú©Ù‡ ÙˆÚ©ØªÙˆØ± Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù…Ø­ØµÙˆÙ„ Ø±Ø§ Ø¨Ø§ Ù†Ø§Ù… Ø¯Ù‚ÛŒÙ‚ Ø¢Ù† Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    """
    if not app_initialized:
        raise HTTPException(status_code=503, detail="Service is not initialized. Please call /startup first.")
    try:
        loop = asyncio.get_running_loop()
        embedding = await loop.run_in_executor(None, get_embedding_by_name_sync, request.product_name)
        if embedding:
            return {"product_name": request.product_name, "embedding": embedding}
        else:
            raise HTTPException(status_code=404, detail=f"Product '{request.product_name}' not found in the database.")
    except Exception as e:
        logger.error(f"Error during debug lookup: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="An error occurred during the debug lookup.")



@app.get("/", summary="Health Check")
async def read_root():
    """Endpoint Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª Ø³Ø±ÙˆÛŒØ³."""
    return {
        "status": "OK" if app_initialized else "Pending Initialization",
        "message": "Server is running. Call /startup to initialize model and DB.",
        "version": API_VERSION,
        "initialized": app_initialized
    }

